# Learning Temporal Sequences

## Table of Contents
1. [Introduction](#introduction)
2. [Biological Eveidence](#biological-evidence)
3. [Simulation](#simulation)
4. [Usage](#usage)


![](https://github.com/nircko/memories_sequences/blob/a5188c0a81877ea75e69678e77ac8f3c6374e7b7/data/simulation.gif)\

## Introduction

This repo simulates a basic AI concept of learning temporal sequences, focusing on the stabilization of sequence states and the interplay between different memory patterns of some "intelligent" system.


## Biological Eveidence

This AI problem of memory sequences has been actually observed by biologists and physiologists in a marine sea slug species known as Tritonia diomedea. This organism is has simple yet effective neural network. Despite possessing a relatively primitive nervous system compared to more complex animals, Tritonia diomedea demonstrates sophisticated escape behaviors when threatened.

The neural network of Tritonia diomedea is composed of a small number of neurons, each playing a critical role in the slug's survival. When the sea slug detects a predator, it initiates a series of rapid, undulating movements to swim away. These movements are governed by rhythmic motor patterns, where each pattern corresponds to a specific deterministic state within a relatively small number of neurons. These states collectively form the escape swimming behavior.


## Simulation
In our simulation, the neural system is modeled in a way that mirrors the state transitions of Tritonia diomedea's neurons. As illustrated in the spin state figure, this simulation can represent both the neural-muscle patterns of Tritonia diomedea and numerical digits. The simulation demonstrates that, given a trigger (such as the presence of a predator), a deterministic temporal sequence of memory is initiated, leading to independent transitions between different states or digits. This provides compelling evidence of how memory sequences operate, whether in primitive creatures like Tritonia diomedea or in more complex systems such as machines or humans.

## Usage
Run the code under src folder. Working from MATLAB 2017 and after.
### All right reserved:
This repository contains material related to the lecture on "Learning Temporal Sequences" presented by Nir Goldfriend on 2017 in the Hebrew University of Jerusalem based on the 1986 work of H. Sompolinsky and I. Kanter in the context of controlling the stabilization of sequences' states. This research laid the groundwork for the theoretical foundations of machine learning in its early days, contributing to some of the most advanced technologies developed in later years.


